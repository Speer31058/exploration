{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-license",
   "metadata": {},
   "source": [
    "# PROJECT 11 뉴스 요약봇 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-barbados",
   "metadata": {},
   "source": [
    "긴 문장을 짦게 요약해주는 요약기를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-gallery",
   "metadata": {},
   "source": [
    "## 1 텍스트 요약(Text Summarization) 이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-thunder",
   "metadata": {},
   "source": [
    "> **텍스트 요약(Text Summarization)**  \n",
    "긴 길이의 문서원문을 핵심 주제만으로 구성된 짧은 요약문장들로 변환하는 것을 의미합니다.  \n",
    "\n",
    "<br/>\n",
    "텍스트 요약은 크게 추출적 요약(Extractive Smmarization)과 추상적 요약(Abstractive Summarizatio)의 두 가지 접근으로 나누어볼 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-comedy",
   "metadata": {},
   "source": [
    "### 1.1 추출적 요약(Extractive Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-impression",
   "metadata": {},
   "source": [
    "추출적 요약은 단어 그대로 원문에서 문장들을 추출해서 요약하는 방식입니다.  \n",
    "딥러닝보다는 주로 전통적인 머신러닝 방식에 속하는 텍스트랭크(TextRank) 와 같은 알고리즘을 사용해서 이 방법을 사용합니다.  \n",
    "\n",
    "*텍스트 랭크 : 유사도를 비교하여 문장과 단어에 가중치를 매겨 하나의 문서를 요약해주는 알고리즘이다.  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;하지만 요약을 한다기 보다는 빈도가 높은 단어와 문장들을 추출해내는 성격이 강하다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-canberra",
   "metadata": {},
   "source": [
    "### 1.2 추상적 요약(Abstractive Summarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-probability",
   "metadata": {},
   "source": [
    "원문으로부터 내용이 요약된 새로운 문장을 생성하는 방식입니다.  \n",
    "RNN, LSTM, GRU 으로 추상적 요약 방식을 구현할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-shark",
   "metadata": {},
   "source": [
    "## 2 인공 신경망으로부터 텍스트 요약  훈련시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-damages",
   "metadata": {},
   "source": [
    "### 2.1 RNN(Recurrent Neural Network)\n",
    "RNN 은 시퀀스 모델입니다. 입력과 출력을 시퀀스 단위로 처리하는 모델입니다.  \n",
    "RNN 은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징을 갖고있습니다.  \n",
    "RNN 은 비교적 짧은 시퀀스에 대해서만 효과를 보이는 단점이 있습니다.  \n",
    "그래서 시퀀스가 점점 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 장기 의존성 문제(Long-Term Dependencies)가 발생합니다.  \n",
    "<br/>\n",
    "*시퀀스 : 연관된 연속의 데이터입니다.*  \n",
    "<br/>  \n",
    "\n",
    "![RNN](RNN.png)\n",
    "*이미지 출처 : http://journal.auric.kr/kiee/XmlViewer/f380862*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-engineering",
   "metadata": {},
   "source": [
    "### 2.2 LSTM(Long Short-Term Memory)\n",
    "RNN 의 장기 의존성 문제를 보완한 RNN 의 일종을 LSTM이라고 합니다.  \n",
    "LSTM 은 은닉층에 복잡한 계산을 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정합니다.  \n",
    "<br/>  \n",
    "\n",
    "![LSTM](LSTM.png)\n",
    "*이미지 출처 : http://journal.auric.kr/kiee/XmlViewer/f380862*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-witch",
   "metadata": {},
   "source": [
    "### 2.3 Seq2Seq(Sequence-to-Sequence)\n",
    "seq2seq 는 두 개의 RNN 아키텍처를 사용하여 입력 시퀀스로부터 출력 시퀀스를 생성해내는 자연어 생성 모델입니다.  \n",
    "seq2seq 는 크게 두 개로 구성된 아키텍처로 구성되는데, 바로 인코더와 디코더입니다.  \n",
    "인코더는 입력 문장의 모든 단어들을 순차적으로 입력받은 뒤에 마지막에 이 모든 단어 정보를 압축해서 하나의 벡터(컨텍스트 벡터)로 만듭니다.  \n",
    "디코더는 인코더에서 만든 벡터를 받아서 단어들을 한 개씩 순차적으로 출력합니다.\n",
    "\n",
    "정리하자면 seq2seq 모델은 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고,  \n",
    "디코더는 이 컨텍스트 벡터를 통해서 출력 시퀀스를 만들어 냅니다.  \n",
    "  \n",
    "![seq2seq](seq2seq.png)\n",
    "*이미지 출처 : https://wikidocs.net/24996*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-spokesman",
   "metadata": {},
   "source": [
    "seq2seq 구조에서 디코더는 시작 토큰 sos가 입력되면, 각 시점마다 단어를 생성하고 이 과정을 종료 토큰 eos를 예측하는 순간까지 멈추지 않습니다.  \n",
    "다시 말해 훈련 데이터의 예측 상대 시퀀스의 앞, 뒤에는 시작 토큰과 종료 토큰을 넣어주는 전처리를 통해 어디서 멈춰야하는지 알려줘야합니다.  \n",
    "  \n",
    "![seq2seq02](seq2seq02.png)  \n",
    "*이미지 출처 : https://wikidocs.net/24996*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-knife",
   "metadata": {},
   "source": [
    "### 2.4 어텐션 메커니즘 (Attention Mechanism)\n",
    "seq2seq 모델은 크게 두 가지 문제점이 있습니다.  \n",
    "\n",
    "> 첫째, 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니 정보 손실이 발생합니다.  \n",
    "둘째, RNN의 고질적인 문제인 기울기 손실 문제가 존재합니다.  \n",
    "\n",
    "그래서 입력 문장이 길면 모델의 성능이 떨어지는 현상이 발생합니다.  \n",
    "이를 보완하기 위해 등장한 기법이 어텐션 메커니즘(Attention Mechanism) 입니다.  \n",
    "  \n",
    "![attention](attention.png)  \n",
    "*이미지 출처 : https://wikidocs.net/22893*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-career",
   "metadata": {},
   "source": [
    "어텐션 메커니즘의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 것입니다.  \n",
    "단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-still",
   "metadata": {},
   "source": [
    "<br/>  \n",
    "\n",
    "**```seq2seq 모델을 통해서 추상적 요약 방식의 텍스트 요약기를 만들어 보도록 하겠습니다.```**\n",
    "\n",
    "<br/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-concert",
   "metadata": {},
   "source": [
    "## 3 데이터 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-somerset",
   "metadata": {},
   "source": [
    "### 3.1 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-doctrine",
   "metadata": {},
   "source": [
    "터미널에서 아래의 명령어를 입력하여 파일을 다운로드 받습니다.  \n",
    "\n",
    "`$ wget https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-heath",
   "metadata": {},
   "source": [
    "### 3.2 모듈과 패키지 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-mother",
   "metadata": {},
   "source": [
    "데이터 전처리를 위해 필요한 모듈과 패키지를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "relative-tennis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aiffel-\n",
      "[nltk_data]     dj41/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')    # nltk 패키지의 불용어 사전 : 요약하는데는 거의 의미가 없는 100여개의 불용어가 미리 정리 되어 있음.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-opportunity",
   "metadata": {},
   "source": [
    "### 3.3 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-amendment",
   "metadata": {},
   "source": [
    "다운로드 받은 데이터는 총 98,401 개의 샘플을 갖고있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "czech-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98401\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv(\"HOME\")+\"/aiffel/exploration/news_summarization/data/news_summary_more.csv\"\n",
    "\n",
    "data = pd.read_csv(data_dir, encoding='iso-8859-1')\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "retired-credits",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upGrad learner switches to career in ML & Al w...   \n",
       "1  Delhi techie wins free food from Swiggy for on...   \n",
       "2  New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3  Aegon life iTerm insurance plan helps customer...   \n",
       "4  Have known Hirani for yrs, what if MeToo claim...   \n",
       "\n",
       "                                                text  \n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1  Kunal Shah's credit card bill payment platform...  \n",
       "2  New Zealand defeated India by 8 wickets in the...  \n",
       "3  With Aegon Life iTerm Insurance plan, customer...  \n",
       "4  Speaking about the sexual harassment allegatio...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-taiwan",
   "metadata": {},
   "source": [
    "데이터는 기사의 본문에 해당되는 text와 기사의 제목에 해당되는 headlines 두 가지 열로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-clothing",
   "metadata": {},
   "source": [
    "## 4 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-playback",
   "metadata": {},
   "source": [
    "### 4.1 중복값 제거하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-chile",
   "metadata": {},
   "source": [
    "데이터의 중복 샘플 유무를 확인한 후, 중복 샘플을 제거하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "devoted-professor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    98280\n",
       "text         98360\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-optimum",
   "metadata": {},
   "source": [
    "98,401 개의 전체 데이터에서 text 에는 98,360 개의 유니크한 데이터가 있고, headlines 에는 98,280개의 유니크한 데이터가 있습니다.  \n",
    "text 달라도 headlines 는 동일할 수 있으므로 text 의 중복 데이터만 제거하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broken-characterization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset = ['text'], inplace = True)\n",
    "\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-birthday",
   "metadata": {},
   "source": [
    "### 4.2 null값 제거하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-concert",
   "metadata": {},
   "source": [
    "데이터에 null 값이 있는지 확인하고, null 값을 가진 데이터는 제거하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-safety",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-developer",
   "metadata": {},
   "source": [
    "데이터에 null 값이 존재하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-providence",
   "metadata": {},
   "source": [
    "### 4.3 텍스트 정규화 및 불용어 제거하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-belle",
   "metadata": {},
   "source": [
    "#### 4.3.1 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-platinum",
   "metadata": {},
   "source": [
    "데이터 안에 들어있는 단어들 중에는 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우가 있습니다.  \n",
    "예를 들어 it'll 과 it will 은 같은데 다른 단어들처럼 간주될 수 있습니다.  \n",
    "그래서 기계를 학습시키기 전에 같은 표현으로 통일시켜주는 작업을 해야합니다. 이를 텍스트 정규화 라고 합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-candidate",
   "metadata": {},
   "source": [
    "정규화 사전을 아래와 같이 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "difficult-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \",len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-afternoon",
   "metadata": {},
   "source": [
    "#### 4.3.2 불용어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-present",
   "metadata": {},
   "source": [
    "데이터에는 자주 등장하지만 자연어 처리를 할때 실질적으로 별로 도움이 되지 않는 단어들이 존재합니다.  \n",
    "이를 불용어 라고 부릅니다. 이러한 불용어는 성능을 저하시킬 수 있으므로 제거를 하는게 좋습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-handy",
   "metadata": {},
   "source": [
    "NLTK 에서 제공하는 불용어 리스트를 참조하여 불용어를 제거할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prepared-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-crash",
   "metadata": {},
   "source": [
    "#### 4.3.3 전처리 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-stations",
   "metadata": {},
   "source": [
    "데이터를 정규화하고, 불용어를 제거하는 작업을 함수로 만들어서 전처리하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "labeled-tablet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everything bought great infact ordered twice third ordered wasfor mother father\n",
      "everything bought was great infact ordered twice and the third ordered wasfor my mother and father\n"
     ]
    }
   ],
   "source": [
    "#데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\",sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "\n",
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(preprocess_sentence(temp_text))\n",
    "print(preprocess_sentence(temp_text, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-bible",
   "metadata": {},
   "source": [
    "위 출력 결과를 통해 temp_text 의 문자열이 전처리 된것을 확인할 수 있습니다.  \n",
    "text 는 불용어를 제거하고, summary 는 불용어를 제거하지 않는다는 것에 주의하시길 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-alexandria",
   "metadata": {},
   "source": [
    "전처리 함수가 잘 작동하는 것을 확인했으니, 훈련 데이터 전체에 대해서 전처리를 수행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "saving-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compliant-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================== 전처리 전 ====================================================\n",
      "Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "\n",
      "==================================================== 전처리 후 ====================================================\n",
      "saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers\n"
     ]
    }
   ],
   "source": [
    "# 전처리 후 출력\n",
    "print('='*52 ,'전처리 전', '='*52)\n",
    "print(data.loc[0, 'text'])\n",
    "print()\n",
    "print('='*52 ,'전처리 후', '='*52)\n",
    "print(clean_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pharmaceutical-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_summary = []\n",
    "\n",
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['headlines']:\n",
    "    clean_summary.append(preprocess_sentence(s, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "chronic-survival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================ 전처리 전 ============================\n",
      "upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "\n",
      "============================ 전처리 후 ============================\n",
      "upgrad learner switches to career in ml al with salary hike\n"
     ]
    }
   ],
   "source": [
    "# 전처리 후 출력\n",
    "print('='*28 ,'전처리 전', '='*28)\n",
    "print(data.loc[0, 'headlines'])\n",
    "print()\n",
    "print('='*28 ,'전처리 후', '='*28)\n",
    "print(clean_summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-coverage",
   "metadata": {},
   "source": [
    "전처리 과정을 거친 후에 다시 한번 빈 값(\" \") 이 있는지 확인하는 것이 좋습니다.  \n",
    "전처리 전에는 데이터가 존재했지만, 전처리 과정에서 문장의 모든 단어가 사라지는 경우가 있을 수 있기때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fitted-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "equipped-samuel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines    0\n",
       "text         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stunning-horizontal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "\n",
    "print('전체 샘플수 :',(len(data)))#데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-antibody",
   "metadata": {},
   "source": [
    "### 4.4 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-childhood",
   "metadata": {},
   "source": [
    "Text와 Summary의 최소, 최대, 평균 길이를 구하고, 길이 분포를 시각화해서 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "molecular-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 60\n",
      "텍스트의 평균 길이 : 35.09968483123221\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.299532330215534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('headlines')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('text')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-lebanon",
   "metadata": {},
   "source": [
    "text의 경우 최소 길이가 1, 최대 길이가 60, 평균 길이는 35 입니다.  \n",
    "시각화 된 그래프로 봤을 때는 대체적으로 50 내외의 길이를 가진다는 것을 확인할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-escape",
   "metadata": {},
   "source": [
    "headlines의 경우 최소 길이가 1, 최대 길이가 16, 그리고 평균 길이가 9 입니다.  \n",
    "그래프로 봤을때, 대체적으로 14 이하의 길이를 가지고 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-wagon",
   "metadata": {},
   "source": [
    "각 컬럼의 최소 길이와 최대 길이의 차이가 크지 않습니다. 그래서 text의 최대 길이는 50으로, headlines의 최대 길이는 14로 설정하겠습니다.  \n",
    "\n",
    "`below_threshold_len` 함수에 훈련 데이터와 샘플의 길이를 입력하면, 우리가 결정한 임의의 길이가 몇%의 샘플까지 포함하는지 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pointed-spell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.9998576657177715\n",
      "전체 샘플 중 길이가 14 이하인 샘플의 비율: 0.9997763318422123\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "\n",
    "text_max_len = 50\n",
    "headlines_max_len = 14\n",
    "\n",
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(headlines_max_len,  data['headlines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-tribute",
   "metadata": {},
   "source": [
    "문장 길이를 각각 50과 14로 패딩하게 되면, 데이터의 샘플들을 전부다 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dried-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98324\n"
     ]
    }
   ],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
    "\n",
    "print('전체 샘플수 :',(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-bloom",
   "metadata": {},
   "source": [
    "최종적으로 **98,401** 개의 데이터가 전처리 과정을 통해서 **98,324** 개의 데이터가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-calgary",
   "metadata": {},
   "source": [
    "### 4.5 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-transmission",
   "metadata": {},
   "source": [
    "seq2seq 모델의 디코더는 시작 토큰을 입력받아 문장을 생성하기 시작하고, 종료 토큰을 예측한 순간 문장 생성을 멈춥니다.  \n",
    "그래서 seq2seq 훈련을 위해서 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-martin",
   "metadata": {},
   "source": [
    "시작 토큰은 'sostoken', 종료 토큰은 'eostoken' 이라고 임의로 정하고, 앞뒤로 추가하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "yellow-values",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "      <td>saurav kant alumnus upgrad iiit pg program mac...</td>\n",
       "      <td>sostoken upgrad learner switches to career in ...</td>\n",
       "      <td>upgrad learner switches to career in ml al wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delhi techie wins free food from swiggy for on...</td>\n",
       "      <td>kunal shah credit card bill payment platform c...</td>\n",
       "      <td>sostoken delhi techie wins free food from swig...</td>\n",
       "      <td>delhi techie wins free food from swiggy for on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>sostoken have known hirani for yrs what if met...</td>\n",
       "      <td>have known hirani for yrs what if metoo claims...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  upgrad learner switches to career in ml al wit...   \n",
       "1  delhi techie wins free food from swiggy for on...   \n",
       "2  new zealand end rohit sharma led india match w...   \n",
       "3  aegon life iterm insurance plan helps customer...   \n",
       "4  have known hirani for yrs what if metoo claims...   \n",
       "\n",
       "                                                text  \\\n",
       "0  saurav kant alumnus upgrad iiit pg program mac...   \n",
       "1  kunal shah credit card bill payment platform c...   \n",
       "2  new zealand defeated india wickets fourth odi ...   \n",
       "3  aegon life iterm insurance plan customers enjo...   \n",
       "4  speaking sexual harassment allegations rajkuma...   \n",
       "\n",
       "                                       decoder_input  \\\n",
       "0  sostoken upgrad learner switches to career in ...   \n",
       "1  sostoken delhi techie wins free food from swig...   \n",
       "2  sostoken new zealand end rohit sharma led indi...   \n",
       "3  sostoken aegon life iterm insurance plan helps...   \n",
       "4  sostoken have known hirani for yrs what if met...   \n",
       "\n",
       "                                      decoder_target  \n",
       "0  upgrad learner switches to career in ml al wit...  \n",
       "1  delhi techie wins free food from swiggy for on...  \n",
       "2  new zealand end rohit sharma led india match w...  \n",
       "3  aegon life iterm insurance plan helps customer...  \n",
       "4  have known hirani for yrs what if metoo claims...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "administrative-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 입력, 디코더의 입력, 레이블을 각각 numpy 타입으로 저장\n",
    "\n",
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-bailey",
   "metadata": {},
   "source": [
    "### 4.6 훈련 데이터와 테스트 데이터 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-values",
   "metadata": {},
   "source": [
    "훈련 데이터와 테스트 데이터는 직접 코딩을 통해서 분리하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "developed-template",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37895 23875 50404 ... 32792 36593 65926]\n"
     ]
    }
   ],
   "source": [
    "# 데이터의 샘플 순서를 섞습니다.\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incomplete-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 78660\n",
      "훈련 레이블의 개수 : 78660\n",
      "테스트 데이터의 개수 : 19664\n",
      "테스트 레이블의 개수 : 19664\n"
     ]
    }
   ],
   "source": [
    "# 섞인 데이터를 8:2의 비율로 훈련 데이터와 테스트 데이터로 분리합니다.\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-worse",
   "metadata": {},
   "source": [
    "### 4.7 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-incident",
   "metadata": {},
   "source": [
    "기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 모든 단어들을 모두 정수로 바꿔줍니다.  \n",
    "이를 위해서 각 단어에 고유한 정수를 맵핑하는 작업이 필요합니다. 이 과정을 단어 집합을 만든다고 표현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-benjamin",
   "metadata": {},
   "source": [
    "Keras의 Tokenizer()를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reflected-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-wells",
   "metadata": {},
   "source": [
    "생성된 단어 집합으로부터 빈도수가 낮은 단어들은 훈련 데이터에서 제외하도록 하겠습니다.  \n",
    "등장 빈도수가 11회 미만인 단어들이 데이터에서 얼만큼의 비중을 차지하는지 확인해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "blank-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 69640\n",
      "등장 빈도가 10번 이하인 희귀 단어의 수: 52157\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 17483\n",
      "단어 집합에서 희귀 단어의 비율: 74.89517518667432\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.88893751100337\n"
     ]
    }
   ],
   "source": [
    "threshold = 11\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-exemption",
   "metadata": {},
   "source": [
    "등장 빈도가 11회 미만인 단어들은 단어 집합에서 약 75% 를 차지합니다.  \n",
    "하지만, 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 약 5% 밖에 되지 않습니다.  \n",
    "그래서 등장 빈도가 10회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거합니다.  \n",
    "단어 집합의 크기를 17,000으로 제한하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "administrative-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4197, 9, 1134, 10, 53, 2563, 37, 63, 9843, 1176, 732, 911, 10237, 1326, 911, 17, 812, 2656, 327, 1967, 99, 45, 272, 10237, 179, 1354, 342, 2094, 9, 1], [3, 13, 993, 80, 2515, 4839, 999, 657, 773, 7252, 56, 592, 55, 9, 14781, 177, 1247, 518, 4569, 469, 5936, 3003, 469, 1247, 1247, 1295, 4713, 15386, 136, 658, 6, 816], [1867, 525, 3052, 9479, 3068, 499, 4007, 1106, 42, 19, 6396, 387, 77, 663, 433, 1311, 7458, 1164, 19, 232, 1867, 525, 8019, 9288, 414, 5102, 29, 12, 5151, 34, 4007, 1106, 3533, 3624, 19, 303, 635, 310, 295]]\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 17000\n",
    "src_tokenizer = Tokenizer(num_words = src_vocab) # 단어 집합의 크기를 20000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성.\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "#잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-soldier",
   "metadata": {},
   "source": [
    "headlines 데이터에 대해서도 동일한 작업을 수행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "second-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "assigned-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 30166\n",
      "등장 빈도가 7번 이하인 희귀 단어의 수: 21363\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 8803\n",
      "단어 집합에서 희귀 단어의 비율: 70.81813962739508\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.951513954550505\n"
     ]
    }
   ],
   "source": [
    "threshold = 8\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "structured-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  [[1, 20, 4683, 6, 69, 7, 6, 548, 2409], [1, 565, 97, 2283, 7677, 6, 1109, 97, 8, 313, 621], [1, 456, 4286, 1654, 51, 7], [1, 75, 5208, 838, 4287, 14, 6479], [1, 314, 4817, 260, 3115, 983, 175]]\n",
      "decoder  [[20, 4683, 6, 69, 7, 6, 548, 2409, 2], [565, 97, 2283, 7677, 6, 1109, 97, 8, 313, 621, 2], [456, 4286, 1654, 51, 7, 2], [75, 5208, 838, 4287, 14, 6479, 2], [314, 4817, 260, 3115, 983, 175, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 8500\n",
    "tar_tokenizer = Tokenizer(num_words = tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "#잘 변환되었는지 확인\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-success",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩 작업이 끝났습니다.  \n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 빈 샘플이 되었을 가능성이 있습니다.  \n",
    "요약문의 길이가 1인 샘플들은 모두 삭제하도록 하겠습니다. (빈 샘플은 시작 토큰과 종료 토큰만 남아있기 때문에 길이가 1인 샘플 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "specialized-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 삭제 전 ==========\n",
      "훈련 데이터의 개수 : 78660\n",
      "훈련 레이블의 개수 : 78660\n",
      "테스트 데이터의 개수 : 19664\n",
      "테스트 레이블의 개수 : 19664\n"
     ]
    }
   ],
   "source": [
    "print('='*10 ,'삭제 전', '='*10)\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "constitutional-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1\n",
      "삭제할 테스트 데이터의 개수 : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj41/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :',len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :',len(drop_test))\n",
    "\n",
    "encoder_input_train = np.delete(encoder_input_train, drop_train, axis=0)\n",
    "decoder_input_train = np.delete(decoder_input_train, drop_train, axis=0)\n",
    "decoder_target_train = np.delete(decoder_target_train, drop_train, axis=0)\n",
    "\n",
    "encoder_input_test = np.delete(encoder_input_test, drop_test, axis=0)\n",
    "decoder_input_test = np.delete(decoder_input_test, drop_test, axis=0)\n",
    "decoder_target_test = np.delete(decoder_target_test, drop_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "spoken-scene",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 삭제 후 ==========\n",
      "훈련 데이터의 개수 : 78659\n",
      "훈련 레이블의 개수 : 78659\n",
      "테스트 데이터의 개수 : 19664\n",
      "테스트 레이블의 개수 : 19664\n"
     ]
    }
   ],
   "source": [
    "print('='*10 ,'삭제 후', '='*10)\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :',len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :',len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :',len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-noise",
   "metadata": {},
   "source": [
    "### 4.8 패딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-yemen",
   "metadata": {},
   "source": [
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해주어야 합니다.  \n",
    "아까 정해두었던 최대 길이로 패딩하도록 하겠습니다. (= 최대 길이보다 짧은 데이터들 뒤의 공간에 숫자 0을 넣어 최대 길이로 맞추기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "loose-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================== 패딩 전 ====================================================\n",
      "encoder_input_train\n",
      "[4197, 9, 1134, 10, 53, 2563, 37, 63, 9843, 1176, 732, 911, 10237, 1326, 911, 17, 812, 2656, 327, 1967, 99, 45, 272, 10237, 179, 1354, 342, 2094, 9, 1]\n",
      "\n",
      "encoder_input_test\n",
      "[4112, 345, 147, 318, 212, 8837, 4105, 1134, 12230, 456, 212, 187, 78, 7, 345, 147, 129, 1571, 143, 255, 809, 212, 40, 147, 8425, 3693, 4105, 328]\n",
      "\n",
      "decoder_input_train\n",
      "[1, 20, 4683, 6, 69, 7, 6, 548, 2409]\n",
      "\n",
      "decoder_target_train\n",
      "[20, 4683, 6, 69, 7, 6, 548, 2409, 2]\n",
      "\n",
      "decoder_input_test\n",
      "[1, 3708, 521, 15, 527, 6, 5776, 515, 3947]\n",
      "\n",
      "decoder_target_test\n",
      "[3708, 521, 15, 527, 6, 5776, 515, 3947, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('='*52 ,'패딩 전', '='*52)\n",
    "print('encoder_input_train')\n",
    "print(encoder_input_train[0])\n",
    "print()\n",
    "print('encoder_input_test')\n",
    "print(encoder_input_test[0])\n",
    "print()\n",
    "print('decoder_input_train')\n",
    "print(decoder_input_train[0])\n",
    "print()\n",
    "print('decoder_target_train')\n",
    "print(decoder_target_train[0])\n",
    "print()\n",
    "print('decoder_input_test')\n",
    "print(decoder_input_test[0])\n",
    "print()\n",
    "print('decoder_target_test')\n",
    "print(decoder_target_test[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fabulous-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen = text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen = text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen = headlines_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen = headlines_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen = headlines_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen = headlines_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "printable-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================== 패딩 후 ====================================================\n",
      "encoder_input_train\n",
      "[ 4197     9  1134    10    53  2563    37    63  9843  1176   732   911\n",
      " 10237  1326   911    17   812  2656   327  1967    99    45   272 10237\n",
      "   179  1354   342  2094     9     1     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "\n",
      "encoder_input_test\n",
      "[ 4112   345   147   318   212  8837  4105  1134 12230   456   212   187\n",
      "    78     7   345   147   129  1571   143   255   809   212    40   147\n",
      "  8425  3693  4105   328     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "\n",
      "decoder_input_train\n",
      "[   1   20 4683    6   69    7    6  548 2409    0    0    0    0    0]\n",
      "\n",
      "decoder_target_train\n",
      "[  20 4683    6   69    7    6  548 2409    2    0    0    0    0    0]\n",
      "\n",
      "decoder_input_test\n",
      "[   1 3708  521   15  527    6 5776  515 3947    0    0    0    0    0]\n",
      "\n",
      "decoder_target_test\n",
      "[3708  521   15  527    6 5776  515 3947    2    0    0    0    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('='*52 ,'패딩 후', '='*52)\n",
    "print('encoder_input_train')\n",
    "print(encoder_input_train[0])\n",
    "print()\n",
    "print('encoder_input_test')\n",
    "print(encoder_input_test[0])\n",
    "print()\n",
    "print('decoder_input_train')\n",
    "print(decoder_input_train[0])\n",
    "print()\n",
    "print('decoder_target_train')\n",
    "print(decoder_target_train[0])\n",
    "print()\n",
    "print('decoder_input_test')\n",
    "print(decoder_input_test[0])\n",
    "print()\n",
    "print('decoder_target_test')\n",
    "print(decoder_target_test[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-tuning",
   "metadata": {},
   "source": [
    "## 5 모델 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-toolbox",
   "metadata": {},
   "source": [
    "### 5.1 인코더 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-blues",
   "metadata": {},
   "source": [
    "함수형 API를 이용해서 인코더를 설계해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bearing-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 256\n",
    "hidden_size = 512\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-polish",
   "metadata": {},
   "source": [
    "### 5.2 디코더 설계하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-destruction",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일합니다.  \n",
    "하지만 LSTM의 입력을 정의할 때, initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "above-raleigh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences = True, return_state = True, dropout = 0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = [state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-martial",
   "metadata": {},
   "source": [
    "디코더의 출력층을 설계하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "flush-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 256)      4352000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 512), (N 1574912     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 512), (N 2099200     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2176000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 512), (N 2099200     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 512),  1574912     embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8500)   4360500     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 18,236,724\n",
      "Trainable params: 18,236,724\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation = 'softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-format",
   "metadata": {},
   "source": [
    "### 5.3 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-endorsement",
   "metadata": {},
   "source": [
    "이미 구현된 어텐션 함수를 가져와서 디코더의 출력층과 결합해 보도록 하겠습니다.  \n",
    "<br/>\n",
    "\n",
    "아래의 코드를 수행하여 깃허브에 공개되어 있는 어텐션 함수를 다운로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "rational-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/thushv89/attention_keras/master/src/layers/attention.py\", filename=\"attention.py\")\n",
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-portal",
   "metadata": {},
   "source": [
    "어텐션 메커니즘을 사용하기 위해 설계한 디코더의 출력층을 수정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "explicit-vault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 256)      4352000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50, 512), (N 1574912     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50, 512), (N 2099200     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    2176000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 50, 512), (N 2099200     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 512),  1574912     embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 512),  524800      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 1024)   0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8500)   8712500     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 23,113,524\n",
      "Trainable params: 23,113,524\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis = -1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-moderator",
   "metadata": {},
   "source": [
    "## 6 모델 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-filename",
   "metadata": {},
   "source": [
    "설계한 모델을 가지고 훈련을 진행하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-commons",
   "metadata": {},
   "source": [
    "`EarlyStopping` 을 사용하여, 특정 조건이 충족되면 모델의 훈련이 멈추도록 하겠습니다.  \n",
    "검증 데이터의 손실을 모니터링 하면서, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 관측되면 학습을 멈추도록 설정합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "miniature-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "615/615 [==============================] - 305s 497ms/step - loss: 4.5373 - val_loss: 4.0731\n",
      "Epoch 2/50\n",
      "615/615 [==============================] - 305s 496ms/step - loss: 3.8688 - val_loss: 3.6582\n",
      "Epoch 3/50\n",
      "615/615 [==============================] - 299s 486ms/step - loss: 3.5196 - val_loss: 3.4168\n",
      "Epoch 4/50\n",
      "615/615 [==============================] - 300s 488ms/step - loss: 3.2802 - val_loss: 3.2853\n",
      "Epoch 5/50\n",
      "615/615 [==============================] - 300s 488ms/step - loss: 3.0951 - val_loss: 3.1668\n",
      "Epoch 6/50\n",
      "615/615 [==============================] - 300s 488ms/step - loss: 2.9485 - val_loss: 3.0942\n",
      "Epoch 7/50\n",
      "615/615 [==============================] - 300s 488ms/step - loss: 2.8265 - val_loss: 3.0490\n",
      "Epoch 8/50\n",
      "615/615 [==============================] - 300s 488ms/step - loss: 2.7253 - val_loss: 3.0098\n",
      "Epoch 9/50\n",
      "615/615 [==============================] - 286s 465ms/step - loss: 2.6332 - val_loss: 2.9793\n",
      "Epoch 10/50\n",
      "615/615 [==============================] - 270s 439ms/step - loss: 2.5523 - val_loss: 2.9603\n",
      "Epoch 11/50\n",
      "615/615 [==============================] - 271s 440ms/step - loss: 2.4823 - val_loss: 2.9461\n",
      "Epoch 12/50\n",
      "615/615 [==============================] - 270s 439ms/step - loss: 2.4208 - val_loss: 2.9328\n",
      "Epoch 13/50\n",
      "615/615 [==============================] - 271s 440ms/step - loss: 2.3668 - val_loss: 2.9334\n",
      "Epoch 14/50\n",
      "615/615 [==============================] - 269s 438ms/step - loss: 2.3179 - val_loss: 2.9312\n",
      "Epoch 15/50\n",
      "615/615 [==============================] - 270s 440ms/step - loss: 2.2699 - val_loss: 2.9327\n",
      "Epoch 16/50\n",
      "615/615 [==============================] - 271s 441ms/step - loss: 2.2186 - val_loss: 2.9248\n",
      "Epoch 17/50\n",
      "615/615 [==============================] - 271s 440ms/step - loss: 2.1736 - val_loss: 2.9301\n",
      "Epoch 18/50\n",
      "615/615 [==============================] - 269s 437ms/step - loss: 2.1337 - val_loss: 2.9252\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 2)\n",
    "history = model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, callbacks=[es], epochs = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-wallace",
   "metadata": {},
   "source": [
    "훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정을 시각화 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "laden-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-death",
   "metadata": {},
   "source": [
    "## 7 인퍼런스 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-rebate",
   "metadata": {},
   "source": [
    "테스트 단계에서 정수 인덱스 행렬로 존재하던 텍스트 데이터를 실제 데이터로 복원해야 하므로, 필요한 3개의 사전을 미리 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "effective-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-helping",
   "metadata": {},
   "source": [
    "seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로, 그에 맞게 모델 설계를 별도로 진행해야합니다.  \n",
    "인코더 모델과 디코더 모델을 분리하여 인퍼런스 모델을 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sublime-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "convertible-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "small-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인퍼런스 단계에서 단어 시퀀스를 완성하는 함수\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-duplicate",
   "metadata": {},
   "source": [
    "## 8 모델 테스트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-paintball",
   "metadata": {},
   "source": [
    "테스트 단계에서 정수 시퀀스를 텍스트 시퀀스로 변환하여 결과를 확인하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-induction",
   "metadata": {},
   "source": [
    "주어진 정수 시퀀스 시퀀스를 텍스트 시퀀스로 변환하는 함수를 만들겠습니다.  \n",
    "함수를 만들 때, 숫자 0(패딩), 시작 토큰, 종료 토큰을 출력에서 제외시키고 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "moved-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-mixture",
   "metadata": {},
   "source": [
    "테스트 데이터 약 50개의 샘플에 대해서 실제 요약과 예측된 요약을 비교해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "imposed-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : russian businessman engaged lawsuit false trump russian ties filed motion titled six ways misled court picture kitten page filing addressed allegedly facts argument get lawsuit dismissed \n",
      "실제 요약 : lawyers file motion on six ways the court \n",
      "예측 요약 :  russian firm sues us over false charges\n",
      "\n",
      "\n",
      "원문 : brihanmumbai municipal corporation reportedly way start online application process speed granting approvals ganpati mumbai process developing software clear year application officials said notably city organisers seek permissions structures \n",
      "실제 요약 : mumbai civic body to approve ganpati online \n",
      "예측 요약 :  mumbai civic body to allow local to\n",
      "\n",
      "\n",
      "원문 : former footballer david beckham criticised calling hong kong china beckham spent two days shanghai hong kong following posted facebook video captioned great hours china although caption later changed user posted government currency even football team \n",
      "실제 요약 : david beckham slammed for to hong kong as china \n",
      "예측 요약 :  david beckham slammed for china post on paris\n",
      "\n",
      "\n",
      "원문 : jammu kashmir police confirmed two militants including lashkar taiba commander bashir killed anantnag following encounter security forces saturday two civilians also killed ten injured one wanted lashkar commanders carried bounty lakh \n",
      "실제 요약 : militants civilians die in in \n",
      "예측 요약 :  militants killed in encounter in jammu and kashmir\n",
      "\n",
      "\n",
      "원문 : year old girl punjab patiala allegedly raped thursday landlord currently absconding girl complained severe stomach ache following mother consulted doctor informed assault woman son told landlord taken girl following filed fir \n",
      "실제 요약 : year old girl raped by in punjab \n",
      "예측 요약 :  girl raped by year old girl in punjab\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000, 10005):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-accreditation",
   "metadata": {},
   "source": [
    "**1. 첫번째 샘플 요약 비교** \n",
    "\n",
    ">실제 요약 : lawyers file motion on six ways the court  \n",
    "예측 요약 : russian firm sues us over false charges \n",
    "\n",
    "원문 내용은 법, 소송과 관련된 내용으로, 무엇인가 '재판' 하고 있다는 점에서 잘 요약한 것 같다.\n",
    "  \n",
    "**2. 두번째 샘플 요약 비교**  \n",
    "\n",
    ">실제 요약 : mumbai civic body to approve ganpati online  \n",
    "예측 요약 : mumbai civic body to allow local to \n",
    "\n",
    "원문 내용은 뭄바이시가 온라인 허가를 위해 힘쓴다는 내용으로, '뭄마이시가 무엇을 한다.'는 점에서 잘 요약한 것 같다. \n",
    "\n",
    "**3. 세번째 샘플 요약 비교**  \n",
    "\n",
    ">실제 요약 : david beckham slammed for to hong kong as china  \n",
    "예측 요약 : david beckham slammed for china post on paris\n",
    "\n",
    "원문 내용은 데이비드 베컴이 중국을 비난했다는 내용으로, '데이비드 배컴이 중국을 향해 등을 돌렸다.'는 점에서 잘 요약한 것 같다.  \n",
    "\n",
    "**4. 네번째 샘플 요약 비교**  \n",
    "\n",
    ">실제 요약 : militants civilians die in in  \n",
    "예측 요약 : militants killed in encounter in jammu and kashmir \n",
    "\n",
    "원문 내용은 경찰이 사람의 목숨을 빼앗았다는 내용으로, '누군가가 죽었다.'는 점에서 잘 요약한 것 같다.  \n",
    "\n",
    "**5. 다섯번째 샘플 요약 비교**  \n",
    "\n",
    ">실제 요약 : year old girl raped by in punjab  \n",
    "예측 요약 : girl raped by year old girl in punjab \n",
    "\n",
    "원문 내용은 여성이 성폭행을 당했다는 내용으로, '여성이 강간당했다.'는 점에서 잘 요약한 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-civilization",
   "metadata": {},
   "source": [
    "## 9 추출적 요약 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-report",
   "metadata": {},
   "source": [
    "추출적 요약은 본문에 존재하는 단어구, 문장을 뽑아서 요약하는 방법입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-authorization",
   "metadata": {},
   "source": [
    "### 9.1 summarize 사용하기\n",
    "패키지 `Summa` 에서는 추출적 요약을 위한 모듈인 `summarize` 를 사용하여 간단하게 추출적 요약을 해보도록 하겠습니다.  \n",
    "`Summa` 의 `summarize()` 의 인자로 사용되는 값들은 아래와 같습니다.  \n",
    "> text (str) : 요약할 텍스트  \n",
    "ratio (float, optional) : 요약문을 생성할때, 원본에서 선택되는 문장 비율  \n",
    "words (int or None, optional) : 출력에 포함할 단어 수\n",
    "split (bool, optional) : True면 문장 list / False 는 조인된 문자열을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "square-importance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "실제 요약 : upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "추출적 요약 : upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "\n",
      "\n",
      "원문 : Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n",
      "실제 요약 : Delhi techie wins free food from Swiggy for one year on CRED\n",
      "추출적 요약 : Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n",
      "\n",
      "\n",
      "원문 : New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\n",
      "실제 요약 : New Zealand end Rohit Sharma-led India's 12-match winning streak\n",
      "추출적 요약 : The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\n",
      "\n",
      "\n",
      "원문 : With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.\n",
      "실제 요약 : Aegon life iTerm insurance plan helps customers save tax\n",
      "추출적 요약 : Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.\n",
      "\n",
      "\n",
      "원문 : Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'.\n",
      "실제 요약 : Have known Hirani for yrs, what if MeToo claims are not true: Sonam\n",
      "추출적 요약 : Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"원문 :\", data.loc[i, ['text']][0])\n",
    "    print(\"실제 요약 :\", data.loc[i, ['headlines']][0])\n",
    "    print(\"추출적 요약 :\", summarize(data.loc[i, ['text']][0], ratio=0.5))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-capability",
   "metadata": {},
   "source": [
    "### 9.2 keywords 사용하기\n",
    "패키지 `Summa` 에서 `keywords` 모듈을 사용하면 텍스트에서 중요한 단어를 추출할 수 있습니다.  \n",
    "`Summa` 의 `keywords()` 을 사용하여 키워드를 추출해보도록 하겠습니다.  \n",
    "`Summa` 의 `keywords()` 의 인자로 사용되는 값들 `summarize()` 의 인자와 같습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "organized-catholic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n",
      "실제 요약 : upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "키워드 : ['saurav', 'kant', 'power', 'powered', 'career', 'careers', 'hike']\n",
      "\n",
      "\n",
      "원문 : Kunal Shah's credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.\n",
      "실제 요약 : Delhi techie wins free food from Swiggy for one year on CRED\n",
      "키워드 : ['saurav', 'kant', 'power', 'powered', 'career', 'careers', 'hike']\n",
      "\n",
      "\n",
      "원문 : New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma's captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\n",
      "실제 요약 : New Zealand end Rohit Sharma-led India's 12-match winning streak\n",
      "키워드 : ['saurav', 'kant', 'power', 'powered', 'career', 'careers', 'hike']\n",
      "\n",
      "\n",
      "원문 : With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to Ã¢ÂÂ¹46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.\n",
      "실제 요약 : Aegon life iTerm insurance plan helps customers save tax\n",
      "키워드 : ['saurav', 'kant', 'power', 'powered', 'career', 'careers', 'hike']\n",
      "\n",
      "\n",
      "원문 : Speaking about the sexual harassment allegations against Rajkumar Hirani, Sonam Kapoor said, \"I've known Hirani for many years...What if it's not true, the [#MeToo] movement will get derailed.\" \"In the #MeToo movement, I always believe a woman. But in this case, we need to reserve our judgment,\" she added. Hirani has been accused by an assistant who worked in 'Sanju'.\n",
      "실제 요약 : Have known Hirani for yrs, what if MeToo claims are not true: Sonam\n",
      "키워드 : ['saurav', 'kant', 'power', 'powered', 'career', 'careers', 'hike']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from summa.keywords import keywords\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"원문 :\", data.loc[i, ['text']][0])\n",
    "    print(\"실제 요약 :\", data.loc[i, ['headlines']][0])\n",
    "    print(\"키워드 :\", keywords(data.loc[0, ['text']][0], words = 5, split=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-posting",
   "metadata": {},
   "source": [
    "추출적 요약을 통해 요약한 문장을 보면 원본에 있는 문장을 그대로 가져와서 요약을 하거나, 원본에 있는 단어를 그대로 가져와서 키워드로 추출합니다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-visibility",
   "metadata": {},
   "source": [
    "| | 추상적 요약 | 추출적 요약 |\n",
    "|:----------|:----------|:----------|\n",
    "| 정의 | 원문에 없던 문장이라도 핵심 문맥을 반영한 새로운 문장을 생성해서 원문을 요약하는 방법 | 원문에서 중요한 핵심 문장이나 단어를 몇 개 뽑아서 요약문을 만드는 방법 |\n",
    "| 난이도 | 상 | 하 |\n",
    "| 방법 | seq2seq | Summa 의 summarize(), keywords() |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-flight",
   "metadata": {},
   "source": [
    "# [ 결과 - 루브릭 ]\n",
    "**1. Abstractive 모델 구성을 위한 텍스트 전처리 단계가 체계적으로 진행되었다.**\n",
    "- 분석단계, 정제단계, 정규화와 불용어 제거, 데이터셋 분리, 인코딩 과정이 빠짐없이 체계적으로 진행되었습니다. :)  \n",
    "\n",
    "**2. 텍스트 요약모델이 성공적으로 학습되었음을 확인하였다.** \n",
    "- 모델학습이 안정적으로 수렴되었음을 그래프를 통해 확인하였으며, 실제 요약문과 유사한 요약문장을 얻을 수 있었습니다. :)\n",
    "  \n",
    "**3. Extractive 요약을 시도해 보고 Abstractive 요약 결과와 함께 비교해 보았다.**\n",
    "- 두 요약 결과를 비교해 보았습니다. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.475px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
