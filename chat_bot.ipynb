{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alert-shark",
   "metadata": {},
   "source": [
    "# PROJECT 15 트랜스포머로 만드는 대화형 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-future",
   "metadata": {},
   "source": [
    "**트랜스 포머 모델을 기반으로 한 인코더-디코더 구조를 바탕으로 챗봇을 제작해 보도록 하겠습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-oasis",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "foreign-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charitable-engine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('ChatbotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-dutch",
   "metadata": {},
   "source": [
    "ChatbotData.csv 데이터는 질문과 대답의 쌍으로 이루어진 데이터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guilty-desert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data 개수 : 11823\n"
     ]
    }
   ],
   "source": [
    "print('train data 개수 :', len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-dodge",
   "metadata": {},
   "source": [
    "train data 개수는 총 11,823개 입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-briefing",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-spider",
   "metadata": {},
   "source": [
    "데이터 안에 Null 값이 있는지 확인해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handy-shoot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q        0\n",
      "A        0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-metallic",
   "metadata": {},
   "source": [
    "Null 값은 존재하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-intention",
   "metadata": {},
   "source": [
    "데이터에서 ?, ., ! 와 같은 구두점 앞에 공백을 추가하여 다른 문자들과 구분하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_strip(train_data):\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for q,a in zip(train_data['Q'], train_data['A']):\n",
    "        q = re.sub(r\"([?.!,])\", r\" \\1 \", q)\n",
    "        a = re.sub(r\"([?.!,])\", r\" \\1 \", a)\n",
    "        \n",
    "        q = q.strip()\n",
    "        a = a.strip()\n",
    "        \n",
    "        questions.append(q)\n",
    "        answers.append(a)\n",
    "        \n",
    "    return questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developing-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = preprocess_strip(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hybrid-history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Question ====================\n",
      "12시 땡! → 12시 땡 !\n",
      "1지망 학교 떨어졌어 → 1지망 학교 떨어졌어\n",
      "3박4일 놀러가고 싶다 → 3박4일 놀러가고 싶다\n",
      "3박4일 정도 놀러가고 싶다 → 3박4일 정도 놀러가고 싶다\n",
      "PPL 심하네 → PPL 심하네\n",
      "\n",
      "==================== Answer ====================\n",
      "하루가 또 가네요. → 하루가 또 가네요 .\n",
      "위로해 드립니다. → 위로해 드립니다 .\n",
      "여행은 언제나 좋죠. → 여행은 언제나 좋죠 .\n",
      "여행은 언제나 좋죠. → 여행은 언제나 좋죠 .\n",
      "눈살이 찌푸려지죠. → 눈살이 찌푸려지죠 .\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20 + \" Question \" + \"=\"*20)\n",
    "for i in range(5):\n",
    "    print(f\"{train_data['Q'][i]} → {questions[i]}\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*20 + \" Answer \" + \"=\"*20)\n",
    "for i in range(5):\n",
    "    print(f\"{train_data['A'][i]} → {answers[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-rover",
   "metadata": {},
   "source": [
    "구두점 앞에 띄어쓰기가 추가되어 분리된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-nitrogen",
   "metadata": {},
   "source": [
    "## 3. 단어 집합 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-elephant",
   "metadata": {},
   "source": [
    "서브워드텍스트인코더를 사용하여 질문, 답변 데이터로부터 단어 집합을 생성하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "outside-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honey-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8178\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-cleaning",
   "metadata": {},
   "source": [
    "8,178 개의 단어 집합을 생성하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-explanation",
   "metadata": {},
   "source": [
    "시작은 의미하는 시작 토큰 sos 와 종료를 의미하는 종료 토큰 eos 를 단어 집합에 포함시키도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flexible-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "north-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : 8178\n",
      "종료 토큰 번호 : 8179\n",
      "단어 집합의 크기 : 8180\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',START_TOKEN[0])\n",
    "print('종료 토큰 번호 :',END_TOKEN[0])\n",
    "print('단어 집합의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-moore",
   "metadata": {},
   "source": [
    "sos 토큰과 eos 토큰을 추가함으로써 단어 집합의 크기가 8,178 개에서 8,180 개로 늘어났습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-logic",
   "metadata": {},
   "source": [
    "## 4. 정수 인코딩과 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-printer",
   "metadata": {},
   "source": [
    "tokenizer.encode() 를 사용하여 정수 인코딩을 하도록 하겠습니다.  \n",
    "패딩의 길이는 40으로 설정하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordinary-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legendary-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question shape : (11823, 40)\n",
      "answer shape : (11823, 40)\n"
     ]
    }
   ],
   "source": [
    "print('question shape :', questions.shape)\n",
    "print('answer shape :', answers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "funny-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== questions[0] ==============================\n",
      "[8178 7915 4207 3060   41 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "============================== answers[0] ==============================\n",
      "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 0번 샘플을 임의로 출력\n",
    "print(\"=\"*30 + \" questions[0] \" + \"=\"*30)\n",
    "print(questions[0])\n",
    "print()\n",
    "\n",
    "print(\"=\"*30 + \" answers[0] \" + \"=\"*30)\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-prince",
   "metadata": {},
   "source": [
    "## 5. 인코더와 디코더의 입력, 레이블 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-guest",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices() 를 사용하여 tf.data.Dataset 를 생성합니다.  \n",
    "tf.data.Dataset 을 생성할 때, 인코더의 입력은 토큰 그대로 하고, 인코더의 출력은 맨 처음 토큰을 제거합니다. 그리고 디코더의 입력은 마지막 토큰을 제거합니다.  \n",
    "이렇게 설정해야 교사 강요를 사용할 수 있습니다,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "verbal-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,    # 디코더의 입력\n",
    "        'dec_inputs': answers[:, :-1] # 디코더의 입력, 인코더의 출력\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 디코더의 출력\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "center-nancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== 원천 데이터 ==============================\n",
      "[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "\n",
      "======================= 인코더의 출력 및 디코더의 입력 =======================\n",
      "[[8178 3844   74 7894    1 8179    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "============================== 디코더의 출력 ==============================\n",
      "[[3844   74 7894    1 8179    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*30 + \" 원천 데이터 \" + \"=\"*30)\n",
    "print(answers[0])\n",
    "print()\n",
    "\n",
    "print(\"=\"*23 + \" 인코더의 출력 및 디코더의 입력 \" + \"=\"*23)\n",
    "print(answers[:1][:, :-1])\n",
    "print()\n",
    "\n",
    "print(\"=\"*30 + \" 디코더의 출력 \" + \"=\"*30)\n",
    "print(answers[:1][:, 1:])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-spelling",
   "metadata": {},
   "source": [
    "## 6. 포지셔널 인코딩 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-substitute",
   "metadata": {},
   "source": [
    "트랜스 포머는 단어 입력을 순차적으로 받는 방식이 아니기 때문에 단어의 위치 정보를 알아야할 필요가 있습니다. 트랜스 포머는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보를 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩이라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personalized-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    angle_rads = np.zeros(angle_rads.shape)\n",
    "    angle_rads[:, 0::2] = sines\n",
    "    angle_rads[:, 1::2] = cosines\n",
    "    pos_encoding = tf.constant(angle_rads)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    print(pos_encoding.shape)\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-bridal",
   "metadata": {},
   "source": [
    "## 7. 스케일 닷-프로덕트 어텐션 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-prior",
   "metadata": {},
   "source": [
    "트랜스 포머에서는 어텐션 값을 구하는 방법으로 스케일 닷 프로덕트 어텐션을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "connected-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
    "\n",
    "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 스케일링\n",
    "  # dk의 루트값으로 나눠준다.\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
    "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
    "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-interface",
   "metadata": {},
   "source": [
    "## 8. 멀티 헤드 어텐션 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-prevention",
   "metadata": {},
   "source": [
    "스케일 닷-프로덕트 어텐션을 여러 개 만들어 다양한 특징에 대한 어텐션을 볼 수 있게 멀티 헤드 어텐션을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "romantic-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    # d_model을 num_heads로 나눈 값.\n",
    "    # 논문 기준 : 64\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    # WO에 해당하는 밀집층 정의\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
    "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 4. 헤드 연결(concatenate)하기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 5. WO에 해당하는 밀집층 지나기\n",
    "    # (batch_size, query의 문장 길이, d_model)\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-dynamics",
   "metadata": {},
   "source": [
    "## 9. 패딩 마스크 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-hughes",
   "metadata": {},
   "source": [
    "패딩은 문장의 길이가 서로 다를 때, 모든 문장의 길이를 동일하게 해주는 패딩 마스크 과정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "leading-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, key의 문장 길이)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-nursery",
   "metadata": {},
   "source": [
    "## 10. 룩 어헤드 마스킹 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-cricket",
   "metadata": {},
   "source": [
    "트랜스 포머는 전체 문장이 문장 행렬로 들어가기 때문에 위치와 상관없이 모든 단어를 참고해서 다음 단어를 예측합니다. 하지만 우리가 원하는 것은 이전 단어들로부터 다음 담어를 예측하는 것이기 때문에, 다음에 나올 단어를 참고하지 않도록 가리는 기법인 룩 어헤드 마스킹을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "critical-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-norfolk",
   "metadata": {},
   "source": [
    "## 11. 인코더 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-concert",
   "metadata": {},
   "source": [
    "하나의 인코더 층은 셀프 어텐션과 피드 포워드 신경망으로 이루어져있습니다. 셀프 어텐션은 멀티 헤드 어텐션으로 병렬적으로 이루어집니다.  \n",
    "셀프 어텐션과 피드 포워드 신경망을 가지는 하나의 인코더 층을 구현하는 함수는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "urban-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': padding_mask # 패딩 마스크 사용\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-participant",
   "metadata": {},
   "source": [
    "위에서 구현한 인코더 층을 임베딩 층과 포지셔널 포지셔널 인코딩을 연결하고, 인코더 층을 쌓아 트랜스 포머의 인코더를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "decreased-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 인코더는 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 인코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-scientist",
   "metadata": {},
   "source": [
    "## 12. 디코더 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-reporter",
   "metadata": {},
   "source": [
    "하나의 디코더 층은 셀프 어텐션, 인코더-디코더 어텐션, 피드 포워드 신경망으로 이루어져있습니다.  \n",
    "셀프 어텐션, 인코더-디코더는 멀티 헤드 어텐션으로 병렬적으로 수행합니다.  \n",
    "셀프 어텐션, 인코더-디코더 어텐션, 피드 포워드 신경망을 가지는 하나의 디코더 층을 구현하는 함수는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brief-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "  # 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "  # 패딩 마스크(두번째 서브층)\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
    "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
    "      })\n",
    "\n",
    "  # 잔차 연결과 층 정규화\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
    "          'mask': padding_mask # 패딩 마스크\n",
    "      })\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
    "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-eclipse",
   "metadata": {},
   "source": [
    "위에서 구현한 디코더의 층은 임베딩 층과 포지셔널 인코딩을 연결하고, 디코더 층을 쌓아 트랜스 포머의 디코더를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "actual-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff,\n",
    "            d_model, num_heads, dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "\n",
    "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 포지셔널 인코딩 + 드롭아웃\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # 디코더를 num_layers개 쌓기\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
    "        dropout=dropout, name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-intake",
   "metadata": {},
   "source": [
    "## 13. 트랜스포머 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-catholic",
   "metadata": {},
   "source": [
    "앞에서 구현한 인코더 층 함수와 디코더 층 함수를 사용하여 트랜스포머 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "frank-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff,\n",
    "                d_model, num_heads, dropout,\n",
    "                name=\"transformer\"):\n",
    "\n",
    "  # 인코더의 입력\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 디코더의 입력\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더의 패딩 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask, output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 디코더의 패딩 마스크(두번째 서브층)\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
    "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
    "\n",
    "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
    "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
    "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 다음 단어 예측을 위한 출력층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-contract",
   "metadata": {},
   "source": [
    "## 14. 모델 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-grade",
   "metadata": {},
   "source": [
    "하이퍼파라미터의 값은 아래와 같이 설정합니다.  \n",
    "\n",
    "${d}_{model} = 256$  \n",
    "\n",
    "num_layers = 2  \n",
    "\n",
    "num_heads = 8  \n",
    "\n",
    "${d}_{ff} = 512$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "undefined-preference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8180, 256)\n",
      "(1, 8180, 256)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dff=DFF,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-marina",
   "metadata": {},
   "source": [
    "학습률과 옵티마이저를 정의하고 모델을 학습하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "occasional-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efficient-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "announced-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # 레이블의 크기는 (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "herbal-logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 14s 75ms/step - loss: 1.4436 - accuracy: 0.0259\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 14s 75ms/step - loss: 1.1739 - accuracy: 0.0495\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 14s 74ms/step - loss: 1.0054 - accuracy: 0.0506\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 14s 75ms/step - loss: 0.9308 - accuracy: 0.0541\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 14s 75ms/step - loss: 0.8724 - accuracy: 0.0575\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 13s 73ms/step - loss: 0.8126 - accuracy: 0.0619\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.7465 - accuracy: 0.0679\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.6723 - accuracy: 0.0758\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.5928 - accuracy: 0.0843\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.5101 - accuracy: 0.0936\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.4268 - accuracy: 0.1041\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.3473 - accuracy: 0.1148\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.2728 - accuracy: 0.1254\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.2067 - accuracy: 0.1356\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.1535 - accuracy: 0.1450\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.1096 - accuracy: 0.1533\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0798 - accuracy: 0.1586\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0615 - accuracy: 0.1618\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0508 - accuracy: 0.1636\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0455 - accuracy: 0.1644\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0417 - accuracy: 0.1652\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0401 - accuracy: 0.1652\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0367 - accuracy: 0.1659\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0317 - accuracy: 0.1672\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0278 - accuracy: 0.1681\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0244 - accuracy: 0.1690\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0214 - accuracy: 0.1698\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0203 - accuracy: 0.1700\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0191 - accuracy: 0.1704\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 13s 72ms/step - loss: 0.0165 - accuracy: 0.1710\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 13s 72ms/step - loss: 0.0159 - accuracy: 0.1712\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 14s 73ms/step - loss: 0.0136 - accuracy: 0.1718\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 14s 73ms/step - loss: 0.0127 - accuracy: 0.1721\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0122 - accuracy: 0.1721\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0115 - accuracy: 0.1722\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0103 - accuracy: 0.1725\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0100 - accuracy: 0.1726\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0097 - accuracy: 0.1728\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 13s 72ms/step - loss: 0.0093 - accuracy: 0.1728\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0089 - accuracy: 0.1729\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 13s 73ms/step - loss: 0.0079 - accuracy: 0.1733\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 14s 74ms/step - loss: 0.0079 - accuracy: 0.1732\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0074 - accuracy: 0.1733\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 14s 73ms/step - loss: 0.0071 - accuracy: 0.1734\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 13s 70ms/step - loss: 0.0070 - accuracy: 0.1734\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 13s 71ms/step - loss: 0.0066 - accuracy: 0.1735\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 13s 68ms/step - loss: 0.0066 - accuracy: 0.1735\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0062 - accuracy: 0.1736\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 13s 69ms/step - loss: 0.0059 - accuracy: 0.1736\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 13s 73ms/step - loss: 0.0052 - accuracy: 0.1738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa65db7f390>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-lesbian",
   "metadata": {},
   "source": [
    "## 15. 챗봇 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fuzzy-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "resistant-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "  prediction = evaluate(sentence)\n",
    "\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "understood-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-sensitivity",
   "metadata": {},
   "source": [
    "학습된 트랜스 포머에 임의로 문장을 넣어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "premium-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 배고파\n",
      "Output: 얼른 맛난 음식 드세요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"배고파\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eleven-flooring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 심심해\n",
      "Output: 노래 불러 드릴까요 ?  북치기박치기 헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥헥\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"심심해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "collected-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 게임하고싶어\n",
      "Output: 게임하세요 !\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"게임하고싶어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "attempted-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 공부하기싫어\n",
      "Output: 공부하면 더 많은 선택을 할 수 있죠 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"공부하기싫어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "particular-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 아파\n",
      "Output: 아프지 않았으면 좋겠어요 .\n"
     ]
    }
   ],
   "source": [
    "output = predict(\"아파\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-clause",
   "metadata": {},
   "source": [
    "# [ 결과 - 루브릭 ]\n",
    "**1. 한국어 전처리를 통해 학습 데이터셋을 구축하였다.**\n",
    "- 공백과 특수문자 처리, 토크나이징, 병렬데이터 구축의 과정이 적절히 진행되었습니다. :)  \n",
    "\n",
    "**2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습을 정상적으로 진행하였다.** \n",
    "- 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적으로 수렴하였습니다. :)\n",
    "  \n",
    "**3. 한국어 입력문장에 대해 한국어로 답변하는 함수를 구현하였다.**\n",
    "- 한국어 입력문장에 그럴듯한 한국어로 답변을 리턴하였습니다. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
